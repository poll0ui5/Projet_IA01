{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d310b5",
   "metadata": {},
   "source": [
    "# Projet IA01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b583d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "heart = pd.read_csv(\"heart.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d2cad",
   "metadata": {},
   "source": [
    "# Analyse exploratoire des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution des variables catégorielles\n",
    "plt.figure()\n",
    "sns.histplot(x='Sex', data=heart)\n",
    "plt.title('Sex distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(x='ChestPainType', data=heart)\n",
    "plt.title('ChestPainType distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(x='RestingECG', data=heart)\n",
    "plt.title('RestingECG distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(x='RestingECG', data=heart)\n",
    "plt.title('RestingECG distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(x='ExerciseAngina', data=heart)\n",
    "plt.title('ExerciseAngina distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(x='ST_Slope', data=heart)\n",
    "plt.title('ST Slope distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les variables catégorielles ont-elles un impact sur HeartDisease ?\n",
    "\n",
    "matrice_corr = heart.corr(numeric_only=True)\n",
    "sns.heatmap(matrice_corr, annot=True)\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=heart['Sex'], hue=heart['HeartDisease'])\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=heart['ChestPainType'], hue=heart['HeartDisease'])\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=heart['RestingECG'], hue=heart['HeartDisease'])\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=heart['ExerciseAngina'], hue=heart['HeartDisease'])\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=heart['ST_Slope'], hue=heart['HeartDisease'])\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=heart['FastingBS'], hue=heart['HeartDisease'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identification des valeurs manquantes et aberrantes :\n",
    "\n",
    "heart['FastingBS'] = heart['FastingBS'].astype(object)\n",
    "heart['HeartDisease'] = heart['HeartDisease'].astype(object)\n",
    "\n",
    "print(\"Number of Duplicates:\", heart.duplicated().sum(), \"\\n\") #nombre de doublon\n",
    "print(\"Nombre de NaN : \\n\" , heart.isna().sum()) #nombre de données manquantes\n",
    "    \n",
    "\n",
    "num_df = heart.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "min_values = num_df.describe().loc['min']\n",
    "\n",
    "print(\"Minimums des colonnes numeriques : \")\n",
    "print(min_values)\n",
    "\n",
    "cat_df = heart.select_dtypes(include=['object'])\n",
    "\n",
    "print(\"Valeurs uniques des variables catégorielles : \\n\")\n",
    "for col in cat_df.columns :\n",
    "    unique_value = cat_df[col].unique()\n",
    "    print(unique_value)\n",
    "\n",
    "sns.boxplot(y=heart['Age'])\n",
    "plt.title(\"Boxplot de l'âge\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(y=heart['Cholesterol'])\n",
    "plt.title(\"Boxplot du Cholesterol\")\n",
    "plt.show()\n",
    "    \n",
    "sns.boxplot(y=heart['RestingBP'])\n",
    "plt.title(\"Boxplot de la tension\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(y=heart['MaxHR'])\n",
    "plt.title(\"Boxplot du maximum de fréquence cardiaque atteind\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(y=heart['Oldpeak'])\n",
    "plt.title(\"Boxplot de la hauteur de la dépression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050a9c2",
   "metadata": {},
   "source": [
    "### Traitons maintenant les données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca19a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nettoyage du dataframe\n",
    "# Supprimer les doublons\n",
    "heart = heart.drop_duplicates()\n",
    "\n",
    "# Remplacer les valeurs aberrantes (0 en valeurs manquantes pour les colonnes numériques)\n",
    "# On peut aussi remplacer par la médianne poru éviter \n",
    "heart['Cholesterol'] = heart['Cholesterol'].replace(0, heart['Cholesterol'][heart['Cholesterol'] != 0].mean())\n",
    "heart['RestingBP'] = heart['RestingBP'].replace(0, heart['RestingBP'][heart['RestingBP'] != 0].mean())\n",
    "\n",
    "\n",
    "# Encoder LabelEncoder pour Sex et ExerciseAngina\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_sex = LabelEncoder()\n",
    "le_angina = LabelEncoder()\n",
    "heart['Sex'] = le_sex.fit_transform(heart['Sex'])\n",
    "heart['ExerciseAngina'] = le_angina.fit_transform(heart['ExerciseAngina'])\n",
    "\n",
    "# One-Hot Encoding pour ChestPainType, RestingECG et ST_Slope\n",
    "heart = pd.get_dummies(heart, columns=['ChestPainType', 'RestingECG', 'ST_Slope'], drop_first=True)\n",
    "\n",
    "print(heart.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e5e9e",
   "metadata": {},
   "source": [
    "### Puis les données abérrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e15ac",
   "metadata": {},
   "source": [
    "##### On a décidé de supprimer les données qui sortaient des box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes numériques\n",
    "numeric_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Calcul des bornes IQR\n",
    "Q1 = heart[numeric_cols].quantile(0.25)\n",
    "Q3 = heart[numeric_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identification des outliers\n",
    "outliers_mask = (heart[numeric_cols] < lower_bound) | (heart[numeric_cols] > upper_bound)\n",
    "\n",
    "# Résultats\n",
    "print(\"Nombre d'outliers par variable (après imputation des 0) :\")\n",
    "print(outliers_mask.sum())\n",
    "\n",
    "total_outlier_rows = outliers_mask.any(axis=1).sum()\n",
    "total_rows = len(heart)\n",
    "percent = (total_outlier_rows / total_rows) * 100\n",
    "\n",
    "\n",
    "# Affichage des bornes\n",
    "print(\"\\nBornes utilisées :\")\n",
    "print(pd.DataFrame({'Lower': lower_bound, 'Upper': upper_bound}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee30c7c",
   "metadata": {},
   "source": [
    "##### En cherchant un petit peu et en annalysant plus en profondeur ces données, on remarque que les outliers détectés par les boîtes à moustache ne sont pas très pertinents. En effet, tous sont très significatif sur l'état du patient. De plus en cherchant sur internet, ces données ne sont pas impossibles mais témoignent justement d'un serieux problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b355f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in numeric_cols: \n",
    "    Q1 = heart[i].quantile(0.25)\n",
    "    Q3 = heart[i].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    if i == 'MaxHR':\n",
    "        print(heart[(heart[i] < lower) | (heart[i] > upper)][[i,'HeartDisease']].sort_values(by=i,ascending=True).head(5))\n",
    "    else:\n",
    "        print(heart[(heart[i] < lower) | (heart[i] > upper)][[i,'HeartDisease']].sort_values(by=i, ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2938344",
   "metadata": {},
   "source": [
    "##### On voit clairement ici que ces valeurs identifiées comme aberrantes ont un sens puisqu'elles traduisent presque toutes une maladie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64121b0b",
   "metadata": {},
   "source": [
    "## Train/Test split + standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Y = heart['HeartDisease']\n",
    "X = heart.drop(['HeartDisease'], axis = 1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state=0, stratify=Y)\n",
    "\n",
    "# Question : est-ce qu'on utulise stratify=y qui permet de garder un pourcentage égale de malade dans le teste et train ?\n",
    "# train_test_split(X,Y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# Centrées réduites de notre dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cda708",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644767d",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Y_train = Y_train.astype(int)\n",
    "Y_test = Y_test.astype(int)\n",
    "\n",
    "\n",
    "k_values = range(1, 31)\n",
    "accuracies_test_man = []\n",
    "accuracies_train_man = []\n",
    "\n",
    "accuracies_test_euc=[]\n",
    "accuracies_train_euc=[]\n",
    "\n",
    "for k in k_values:\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k, p=1, metric='minkowski')\n",
    "    knn_classifier.fit(X_train_scaled, Y_train)\n",
    "    y_pred_test = knn_classifier.predict(X_test_scaled)\n",
    "    y_pred_train = knn_classifier.predict(X_train_scaled)\n",
    "    \n",
    "    accuracies_test_man.append(accuracy_score(Y_test,y_pred_test))\n",
    "    accuracies_train_man.append(accuracy_score(Y_train, y_pred_train))\n",
    "\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k, p=2, metric='minkowski')\n",
    "    knn_classifier.fit(X_train_scaled, Y_train)\n",
    "    y_pred_test = knn_classifier.predict(X_test_scaled)\n",
    "    y_pred_train = knn_classifier.predict(X_train_scaled)\n",
    "    \n",
    "    accuracies_test_euc.append(accuracy_score(Y_test,y_pred_test))\n",
    "    accuracies_train_euc.append(accuracy_score(Y_train, y_pred_train))\n",
    "\n",
    "# Visualiser la précision en fonction de k\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(k_values, accuracies_test_man, marker='x', label = 'Test_man',color='red')\n",
    "plt.plot(k_values, accuracies_train_man, marker='x',linestyle='--', label = 'Train_man',color='red',alpha=0.5)\n",
    "plt.plot(k_values, accuracies_train_euc, marker='x',linestyle='--', label = 'Train_euc', color='blue',alpha=0.5)\n",
    "plt.plot(k_values, accuracies_test_euc, marker='x', label = 'Test_euc',color='blue')\n",
    "plt.title('Exactitude du modèle en fonction de k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Exactitude')\n",
    "plt.xticks(k_values)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab053da9",
   "metadata": {},
   "source": [
    "##### On a d'abord utilisé la métrique de l'accuracy pour choisir le meilleur k sauf qu'en réalité, dans le contexte médical il est plus important de maximiser le recall quite à perdre un peu en accurcy. C'est pour cela que l'on a par la suite introduit le F2-Score qui donne deux fois plus de poids au recall qu'à l'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import fbeta_score # On importe la métrique\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Conversion préventive des cibles en entiers\n",
    "Y_train = Y_train.astype(int)\n",
    "Y_test = Y_test.astype(int)\n",
    "\n",
    "# Plages de k\n",
    "k_values = range(1, 21)\n",
    "\n",
    "# Listes pour stocker les scores F2\n",
    "f2_test_man = []\n",
    "f2_train_man = []\n",
    "f2_test_euc = []\n",
    "f2_train_euc = []\n",
    "\n",
    "for k in k_values:\n",
    "    # --- Modèle 1 : Manhattan (p=1) ---\n",
    "    knn_man = KNeighborsClassifier(n_neighbors=k, p=1, metric='minkowski')\n",
    "    knn_man.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    pred_test_man = knn_man.predict(X_test_scaled)\n",
    "    pred_train_man = knn_man.predict(X_train_scaled)\n",
    "    \n",
    "    # Calcul F2 (beta=2 donne 2x plus de poids au Rappel qu'à la Précision)\n",
    "    f2_test_man.append(fbeta_score(Y_test, pred_test_man, beta=2))\n",
    "    f2_train_man.append(fbeta_score(Y_train, pred_train_man, beta=2))\n",
    "\n",
    "    # --- Modèle 2 : Euclidienne (p=2) ---\n",
    "    knn_euc = KNeighborsClassifier(n_neighbors=k, p=2, metric='minkowski')\n",
    "    knn_euc.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    pred_test_euc = knn_euc.predict(X_test_scaled)\n",
    "    pred_train_euc = knn_euc.predict(X_train_scaled)\n",
    "    \n",
    "    # Calcul F2\n",
    "    f2_test_euc.append(fbeta_score(Y_test, pred_test_euc, beta=2))\n",
    "    f2_train_euc.append(fbeta_score(Y_train, pred_train_euc, beta=2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(k_values, f2_test_man, marker='o', label='Test Manhattan (F2)', color='red')\n",
    "plt.plot(k_values, f2_test_euc, marker='o', label='Test Euclidien (F2)', color='blue')\n",
    "\n",
    "plt.plot(k_values, f2_train_man, marker='x', label='Train Manhattan', linestyle='--', color='red', alpha=0.5)\n",
    "plt.plot(k_values, f2_train_euc, marker='x', label='Train Euclidien', linestyle='--', color='blue', alpha=0.5)\n",
    "\n",
    "plt.title('Optimisation du kNN selon le score F2 (Priorité : Détection des malades)')\n",
    "plt.xlabel('k (Nombre de voisins)')\n",
    "plt.ylabel('F2-Score')\n",
    "plt.xticks(k_values)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f2_test_euc[2],f2_test_euc[6],f2_test_euc[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0596a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "knn_man = KNeighborsClassifier(n_neighbors=9, p=1, metric='minkowski')\n",
    "knn_man.fit(X_train_scaled, Y_train)\n",
    "y_pred = knn_man.predict(X_test_scaled)\n",
    "y_pred_train = knn_man.predict(X_train_scaled)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Sain (Pred)', 'Malade (Pred)'],\n",
    "            yticklabels=['Sain (Réel)', 'Malade (Réel)'])\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe prédite')\n",
    "plt.show()\n",
    "\n",
    "print('F2-Score test =',fbeta_score(Y_test,y_pred, beta=2))\n",
    "print('F2-Score train =',fbeta_score(Y_train,y_pred_train, beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470b31b",
   "metadata": {},
   "source": [
    "##### Essayons de chercher si un seuil différent du 'vote majoritaire' changerai quelque chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab73a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paramètres à tester\n",
    "k_values = range(1, 30) # k de 1 à 25\n",
    "thresholds = np.arange(0.1, 0.95, 0.05) # Seuils de 0.10 à 0.90 par pas de 0.05\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Recherche du meilleur couple (k, seuil)...\")\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, p=1) \n",
    "    knn.fit(X_train_scaled, Y_train)\n",
    "    y_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "    best_f2_for_this_k = 0\n",
    "    for t in thresholds:\n",
    "        y_pred_t = (y_prob >= t).astype(int)\n",
    "        score = fbeta_score(Y_test, y_pred_t, beta=2)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'k': k, 'threshold': t}\n",
    "        if score > best_f2_for_this_k:\n",
    "            best_f2_for_this_k = score\n",
    "    results.append(best_f2_for_this_k)\n",
    "\n",
    "print(\"\\n--- RÉSULTAT FINAL ---\")\n",
    "print(f\"Le meilleur F2-Score possible est : {best_score:.4f}\")\n",
    "print(f\"Obtenu avec : k = {best_params['k']} et Seuil = {best_params['threshold']:.2f}\")\n",
    "\n",
    "# Petit graphique pour voir le potentiel de chaque k (avec son meilleur seuil optimisé)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, results, marker='o', color='purple')\n",
    "plt.title('Meilleur F2-Score possible pour chaque k (après optimisation du seuil)')\n",
    "plt.xlabel('k (Nombre de voisins)')\n",
    "plt.ylabel('F2-Score Max')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "best_k = best_params['k']\n",
    "best_thresh = best_params['threshold']\n",
    "\n",
    "knn_final = KNeighborsClassifier(n_neighbors=best_k, p=1)\n",
    "knn_final.fit(X_train_scaled, Y_train)\n",
    "\n",
    "\n",
    "y_probs = knn_final.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_custom = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "y_probs_train = knn_final.predict_proba(X_train_scaled)[:,1]\n",
    "y_pred_train_custom = (y_probs_train >= best_thresh).astype(int)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred_custom)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Sain (0)', 'Malade (1)'],\n",
    "            yticklabels=['Sain (0)', 'Malade (1)'])\n",
    "\n",
    "plt.title(f'Matrice de Confusion Optimisée\\n(k={best_k}, Seuil={best_thresh:.2f})')\n",
    "plt.xlabel('Prédiction du Modèle')\n",
    "plt.ylabel('Vraie Réalité')\n",
    "plt.show()\n",
    "\n",
    "# 6. Affichage des métriques pour confirmer la réussite\n",
    "print(classification_report(Y_test, y_pred_custom))\n",
    "\n",
    "\n",
    "print('F2-Score test =',fbeta_score(Y_test,y_pred_custom, beta=2))\n",
    "print('F2-Score train =',fbeta_score(Y_train,y_pred_train_custom, beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fb628",
   "metadata": {},
   "source": [
    "## Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdcfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model_reg_log = LogisticRegression(C=0.1,max_iter=1000,random_state=42)\n",
    "model_reg_log.fit(X_train_scaled,Y_train)\n",
    "\n",
    "y_pred = model_reg_log.predict(X_test_scaled)\n",
    "y_prob = model_reg_log.predict_proba(X_test_scaled)[:, 1]\n",
    "y_prob_train = model_reg_log.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "print('F2-Score =',fbeta_score(Y_test,y_pred, beta=2))\n",
    "print(f\"AUC-ROC Score     : {roc_auc_score(Y_test, y_prob):.4f}\")\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Sain (Pred)', 'Malade (Pred)'],\n",
    "            yticklabels=['Sain (Réel)', 'Malade (Réel)'])\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe prédite')\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, seuil = roc_curve(Y_test, y_prob)\n",
    "\n",
    "j = tpr - fpr\n",
    "ix = np.argmax(j)\n",
    "meilleur_seuil = seuil[ix]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Régression Logistique (AUC = {roc_auc_score(Y_test, y_prob):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Aléatoire (AUC = 0.5)') # La diagonale du hasard\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='red', s=100, label=(f'Meilleur Seuil{meilleur_seuil:.2f}'))\n",
    "plt.title('Courbe ROC')\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)')\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR / Rappel)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ffdf8",
   "metadata": {},
   "source": [
    "##### Maintenant que nous avons remarqué que le meilleur seuil est à 0.42 au lieu de 0.5 initiallement, on peut recalculer les prédictions et donc les métrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_optimal = (y_prob >= meilleur_seuil).astype(int)\n",
    "y_pred_optimal_train = (y_prob_train >= meilleur_seuil).astype(int)\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(Y_test, y_pred_optimal)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Sain (Pred)', 'Malade (Pred)'],\n",
    "            yticklabels=['Sain (Réel)', 'Malade (Réel)'])\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe prédite')\n",
    "plt.show()\n",
    "\n",
    "print('F2-Score test =',fbeta_score(Y_test,y_pred_optimal, beta=2))\n",
    "print('F2-Score train =',fbeta_score(Y_train,y_pred_optimal_train, beta=2))\n",
    "print(classification_report(Y_test, y_pred_optimal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152fd4e",
   "metadata": {},
   "source": [
    "## Arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909198a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_tree_train = []\n",
    "acc_tree_test = []\n",
    "\n",
    "\n",
    "\n",
    "max_dep = np.arange(1, 20, 1)\n",
    "\n",
    "for maxd in max_dep :\n",
    "    d_tree = DecisionTreeClassifier(max_depth=maxd, random_state=42)\n",
    "    d_tree.fit(X_train, Y_train)\n",
    "    y_tree_train_predict = d_tree.predict(X_train)\n",
    "    y_tree_test_predict = d_tree.predict(X_test)\n",
    "    acc_tree_train.append(accuracy_score(Y_train, y_tree_train_predict))\n",
    "    acc_tree_test.append(accuracy_score(Y_test, y_tree_test_predict))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(max_dep, acc_tree_test, marker='o')\n",
    "plt.plot(max_dep, acc_tree_train, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804af767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# 1. Définir le modèle (avec un random_state pour la reproductibilité)\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 2. Définir la grille des hyperparamètres à tester\n",
    "param_grid = {\n",
    "    # Tester les profondeurs de 3 à 15, par pas de 2\n",
    "    'max_depth': np.arange(1, 16, 1), \n",
    "    \n",
    "    # Tester différents critères de division\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    \n",
    "    # Tester le nombre minimum d'échantillons pour une division\n",
    "    'min_samples_split': [2, 5, 10] \n",
    "}\n",
    "\n",
    "# 3. Initialiser et lancer la recherche par grille\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tree,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall', # Le critère d'évaluation\n",
    "    cv=5,               # Utiliser 5 plis de validation croisée\n",
    "    verbose=1,          # Afficher la progression\n",
    "    n_jobs=-1           # Utiliser tous les cœurs du processeur\n",
    ")\n",
    "\n",
    "# X_train, Y_train sont les données d'entraînement (ne pas utiliser X_test ici)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# 4. Afficher les meilleurs résultats\n",
    "print(\"Meilleurs hyperparamètres:\", grid_search.best_params_)\n",
    "print(\"Meilleur score de CV:\", grid_search.best_score_)\n",
    "\n",
    "# 5. Récupérer le modèle optimal\n",
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "# Utiliser le modèle optimal sur le jeu de test final\n",
    "final_accuracy = best_tree.score(X_test, Y_test)\n",
    "print(f\"Accuracy final sur le jeu de test: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae9582",
   "metadata": {},
   "source": [
    "# Forêts aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#en optimisant l'accuracy\n",
    "\n",
    "forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 2000],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=forest, \n",
    "    param_grid=param_grid, \n",
    "    scoring='accuracy',\n",
    "    cv=5, \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "print(\"Lancement de l'optimisation pour l'ACCURACY...\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Afficher les meilleurs résultats\n",
    "print(\"\\nMeilleurs hyperparamètres trouvés (scoring = 'accuracy') :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\nMeilleur score d'Accuracy (moyenne CV) :\")\n",
    "print(f\"{grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Tester les performances sur l'ensemble de test\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"\\nAccuracy finale sur l'ensemble de test :\")\n",
    "print(f\"{accuracy_score(Y_test, y_test_pred):.4f}\")\n",
    "\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "print(confusion_matrix(Y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642cc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en optimisant le recall\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "forest = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "# Grille de paramètres à tester (max_depth de 1 à la profondeur maximale trouvée)\n",
    "param_grid = {\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 2000],\n",
    " 'max_depth': [5, 10, 20, None],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'max_features': [None, 'sqrt'],\n",
    " 'bootstrap': [True, False]\n",
    "}\n",
    "# Configurer GridSearchCV avec scoring = 'recall' et cv = 5\n",
    "grid_search = GridSearchCV(estimator=forest, param_grid=param_grid, scoring='recall', cv=5, n_jobs=-1)\n",
    "# Entraîner le modèle avec GridSearchCV\n",
    "grid_search.fit(X_train, Y_train)\n",
    "# Afficher les meilleurs résultats\n",
    "\n",
    "print(\"Meilleurs hyperparamètres trouvés (scoring = 'recall') :\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nMeilleur score de recall :\")\n",
    "print(grid_search.best_score_)\n",
    "# Tester les performances sur l'ensemble de test\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"\\nAccuracy sur l'ensemble de test :\")\n",
    "print(accuracy_score(Y_test, y_test_pred))\n",
    "\n",
    "print(confusion_matrix(Y_test, y_test_pred))\n",
    "# Il est également possible de prendre les meilleur paramètre et d'entraîner un nouveau modèle\n",
    "# si on ne veut pas utiliser grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d0214",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#XG Boost ne fonctionne qu'avec des nombres, et non des objets. Il faut donc convertir\n",
    "\n",
    "# 1. Correction spécifique pour FastingBS (qui doit être un entier 0 ou 1)\n",
    "# 'coerce' va transformer les valeurs non convertibles en NaN (que XGBoost gère)\n",
    "\n",
    "X_train_xgb = X_train.copy()\n",
    "X_test_xgb = X_test.copy()\n",
    "\n",
    "X_train_xgb['FastingBS'] = pd.to_numeric(X_train_xgb['FastingBS'], errors='coerce').fillna(0).astype(int)\n",
    "X_test_xgb['FastingBS'] = pd.to_numeric(X_test_xgb['FastingBS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# 2. Encodage automatique de TOUTES les autres colonnes textuelles (Sex, ChestPainType, etc.)\n",
    "# pd.get_dummies transforme \"Sex\" (M/F) en \"Sex_M\" (0/1) et \"Sex_F\" (0/1)\n",
    "print(\"Encodage des variables catégorielles (One-Hot Encoding)...\")\n",
    "X_train_xgb = pd.get_dummies(X_train_xgb)\n",
    "X_test_xgb = pd.get_dummies(X_test_xgb)\n",
    "\n",
    "# Il faut s'assurer que X_train et X_test ont exactement les mêmes colonnes dans le même ordre\n",
    "X_train_xgb, X_test_xgb = X_train_xgb.align(X_test_xgb, join='inner', axis=1)\n",
    "\n",
    "print(\"Nouvelles dimensions des données :\")\n",
    "print(f\"Train : {X_train_xgb.shape}\")\n",
    "print(f\"Test  : {X_test_xgb.shape}\")\n",
    "\n",
    "# Calcul du ratio pour équilibrer les classes (équivalent de class_weight='balanced')\n",
    "count_pos = np.sum(Y_train == 1)\n",
    "count_neg = np.sum(Y_train == 0)\n",
    "scale_ratio = count_neg / count_pos if count_pos > 0 else 1\n",
    "\n",
    "# 1. Configuration du modèle\n",
    "# On utilise binary:logistic car la cible est binaire (0 ou 1)\n",
    "xgb = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_ratio # Force le modèle à donner plus de poids aux positifs\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],      # Nombre d'arbres\n",
    "    'learning_rate': [0.01, 0.05, 0.1],   # Vitesse d'apprentissage \n",
    "    'max_depth': [3, 4, 5],               # Profondeur des arbres \n",
    "    'subsample': [0.8, 1.0],              # Fraction d'échantillons pour chaque arbre\n",
    "    'colsample_bytree': [0.8, 1.0]        # Fraction de colonnes par arbre\n",
    "}\n",
    "\n",
    "# 3. Configuration de la Recherche par Grille (GridSearchCV)\n",
    "# cv=5 : Validation croisée à 5 plis (robuste pour cette taille de données)\n",
    "# scoring='accuracy' ou 'roc_auc' : On cherche à maximiser la précision ici\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall', # Modification ici pour maximiser le Recall\n",
    "    n_jobs=-1,  # Utilise tous les coeurs du processeur\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"--- Démarrage de l'optimisation des hyperparamètres (Grid Search) ---\")\n",
    "\n",
    "# 4. Entraînement sur vos données d'entraînement existantes\n",
    "grid_search.fit(X_train_xgb, Y_train)\n",
    "\n",
    "# 5. Récupération des meilleurs résultats\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\nMeilleurs hyperparamètres trouvés : {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score de validation croisée (Recall) : {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 6. Évaluation finale sur le jeu de test (X_test, y_test)\n",
    "y_pred = best_model.predict(X_test_xgb)\n",
    "\n",
    "# Calcul des métriques\n",
    "final_recall = recall_score(Y_test, y_pred)\n",
    "auc = roc_auc_score(Y_test, best_model.predict_proba(X_test_xgb)[:, 1])\n",
    "\n",
    "print(\"\\n--- PERFORMANCE SUR LE JEU DE TEST ---\")\n",
    "print(f\"Recall Final : {final_recall:.4f}\")\n",
    "print(f\"AUC Score : {auc:.4f}\")\n",
    "print(\"\\nRapport de Classification détaillé :\")\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print(\"Matrice de Confusion :\")\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951441f",
   "metadata": {},
   "source": [
    "## Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import Recall\n",
    "\n",
    "model = Sequential([Input(shape=(X_train.shape[1],)),    \n",
    "                    Dense(16, activation='relu'), \n",
    "                    Dense(1, activation='sigmoid') \n",
    "                    ]) \n",
    "\n",
    "optimizer = Adam(learning_rate = 0.01)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', Recall(name='recall')])\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_scaled,Y_train,epochs=50, batch_size=16, validation_split=0.2, callbacks=[es],verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b20d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_proba = model.predict(X_test_scaled)\n",
    "y_pred_test = (y_pred_test_proba >= 0.5).astype(int)\n",
    "y_pred_train = (model.predict(X_train_scaled)>= 0.5)\n",
    "print(f\"Réseau de neuronnes Test Set F2-Score {fbeta_score(Y_test,y_pred_test,beta=2)}\")\n",
    "print(f\"Réseau de neuronnes Train Set F2-Score {fbeta_score(Y_train,y_pred_train,beta=2)}\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(Y_test, y_pred_test)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Sain (Pred)', 'Malade (Pred)'],\n",
    "            yticklabels=['Sain (Réel)', 'Malade (Réel)'])\n",
    "plt.title('Matrice de Confusion Test')\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe prédite')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b3d4c",
   "metadata": {},
   "source": [
    "##### En abaissant la valeur du seuil à 0.3 voir 0.2 on perd un peu en précision par contre on évite des faux négatifs ce qui nous donnes un F2-Score très élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_proba = model.predict(X_test_scaled)\n",
    "y_pred_test = (y_pred_test_proba >= 0.2).astype(int)\n",
    "y_pred_train = (model.predict(X_train_scaled)>= 0.2)\n",
    "print(f\"Réseau de neuronnes Test Set F2-Score {fbeta_score(Y_test,y_pred_test,beta=2)}\")\n",
    "print(f\"Réseau de neuronnes Train Set F2-Score {fbeta_score(Y_train,y_pred_train,beta=2)}\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(Y_test, y_pred_test)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Sain (Pred)', 'Malade (Pred)'],\n",
    "            yticklabels=['Sain (Réel)', 'Malade (Réel)'])\n",
    "plt.title('Matrice de Confusion Test')\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe prédite')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
